{"cells":[{"metadata":{"_uuid":"1bea9de0d081f1d9fc818ec240a8e3e2ebaef459"},"cell_type":"markdown","source":"# Parsing the property sales data stored in “data.dat”\nThe real estate markets, like those in Sydney and Melbourne, present an interesting opportunity for data analysts to analyze and predict where property prices are moving towards.  Prediction of property prices is becoming increasingly important and beneficial. Property prices are a good indicator of both the overall market condition and the economic health of a country. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues."},{"metadata":{"_uuid":"0d3b96d665f371b49a2758af2e96f3370c4b1b88"},"cell_type":"markdown","source":"### Examining and loading the data into a Pandas DataFrame"},{"metadata":{"trusted":true,"_uuid":"6024248d296c523935a15682895757ba9a1d5c55"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nfrom pandas.io.json import json_normalize\n\n\nwith open(\"../input/data.dat\") as json_file:\n    json_data = json.load(json_file)\n    houses=pd.DataFrame(json_data)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fbdf03c845b0f40b3eeeb3616577e53c50a15463"},"cell_type":"code","source":"#The loaded JSON data has been saved in a Python dictionary. \n#Using json_normalize, flattening the \"data\" dictionary into a table and saving it in a DataFrame \"df\".\ndf=json_normalize(json_data['houses'])\nprint(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013405cff59ccb533ea98498de4ba4084031f0f1"},"cell_type":"code","source":"df['date'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"416409a446144436e4ab479462130ae0ea9912a6"},"cell_type":"markdown","source":"From the above value counts of \"date\", we found that there are two dates with different date format when compared to other date formats."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f94ed7021ad4af36ddef25f67a7bd8a18c054133"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n#plotting bar graph to audit \"date\"\ndf['date'].value_counts().plot(kind=\"line\",figsize=(15,5))\nplt.show()\n\n# there is a different date formate for \"23052014T000000\" replacing it with \"20140523T000000\"\ndf['date'].replace(['23052014T000000'],['20140523T000000'],inplace=True) \n\n# there is a date value with 20140631T000000, where as June month does not contain 31st and hence considering it has Irregularities. changing date 20140631T000000 to 20140701T000000  \ndf['date'].replace(['20140631T000000'],['20140701T000000'],inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9132f63746c7b5f41b435136cfaa997b865aa50"},"cell_type":"markdown","source":"##### Splitting the \"address\" values to => \"street\",\"city\", \"statezip\", \"country\""},{"metadata":{"trusted":true,"_uuid":"9124a03c4c37b7d5150e2de5db2b59511b1d51e2"},"cell_type":"code","source":"#'address' is split according to \",\" and stored into 'address_list'\ndf['address_list'] = df['address'].str.split(', ')\n#from 'address_list', col[0] is asigned to street,col[1] is asigned to city\n#col[2] is asigned to statezip and col[3] is asigned to country\ndf['street'] = df['address_list'].apply(lambda col: col[0])\ndf['city']=df['address_list'].apply(lambda col: col[1])\ndf['statezip']=df['address_list'].apply(lambda col: col[2])\ndf['country']=df['address_list'].apply(lambda col: col[3])\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b476e1e5462e19c5cefc9a27e5c2dfbfabe4d81f"},"cell_type":"code","source":"#Dropping 'address' column and also dummy list created as 'address_list'\ndf.drop('address',axis=1, inplace=True)\ndf.drop('address_list', axis=1, inplace = True)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aab676c07bf522c9cd93c75c3a66cb6727a7524"},"cell_type":"code","source":"#\"room\" attribute values are extracted into 'bathrooms' and 'bedrooms',and \"room\" attribute is dropped once its values are extracted\ndf['bathrooms'] = df.rooms.str.extract('Number of bathrooms: (\\d.\\d+)', expand = True)\ndf['bedrooms'] = df.rooms.str.extract('Number of bedrooms: (\\d+)', expand = True)\ndf.drop('rooms', axis=1, inplace = True)\ndf.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cc741ae111374f633e0aae339edb6a169340b19b"},"cell_type":"code","source":"#splitting the values of 'area.sqft_living/sqft_lot' according to \"=\"\ndf['area.sqft_living/sqft_lot_list'] = df['area.sqft_living/sqft_lot'].str.split('=')\n# col[1] has the values of sqft_living and sqft_lot, hence storing it in 'area.sqft_living/sqft_lot_list_list1'\ndf['area.sqft_living/sqft_lot_list_list1'] = df['area.sqft_living/sqft_lot_list'].apply(lambda col: col[1])\ndf['area.sqft_living/sqft_lot_list_list2'] = df['area.sqft_living/sqft_lot_list_list1'].str.split('\\ ')\ndf['sqft_living']=df['area.sqft_living/sqft_lot_list_list2'].apply(lambda col: col[0])\ndf['sqft_lot']=df['area.sqft_living/sqft_lot_list_list2'].apply(lambda col: col[1])\n#dropping all dummy list used to store while splitting values of 'area.sqft_living/sqft_lot'\ndf.drop('area.sqft_living/sqft_lot_list_list1',axis=1, inplace=True)\ndf.drop('area.sqft_living/sqft_lot_list_list2', axis=1, inplace = True)\ndf.drop('area.sqft_living/sqft_lot_list', axis=1, inplace = True)\ndf.drop('area.sqft_living/sqft_lot', axis=1, inplace = True)\n\n#renaming the columns from \"area.sqft_above\" to \"sqft_above\" and \"area.sqft_basement\" to \"sqft_basement\"\ndf.rename(index=str, columns={\"area.sqft_above\": \"sqft_above\", \"area.sqft_basement\": \"sqft_basement\"},inplace=True)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5aaa963a652dccacf233e4466d168da68b0c1cca"},"cell_type":"code","source":"#right stripping the value of 'sqft_living'\ndf['sqft_living'] = df['sqft_living'].map(lambda x: x.rstrip('\\\\'))\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49efa83726fa5c99d1377d3d912e93cd95b59364"},"cell_type":"code","source":"#Trying find the Irregularities in sqft_living by applying, sqft_basement + sqft_above = sqft_living\ndf['sqft_temp'] =  df[['sqft_basement', 'sqft_above']].sum(axis=1)\n\n#comparing the temp values with 'sqft_living'\ndf[df['sqft_temp'] != df['sqft_living']].index\n#print(df.iloc[[4338]])\n\n#df.ix[4338, 'sqft_living'] = df.ix[4338,['sqft_basement', 'sqft_above']].sum()\n\n#print(df.iloc[[4338]])\n#print(df.iloc[[4339]])\n\n#df.ix[4339, 'sqft_living'] = df.ix[4339,['sqft_basement', 'sqft_above']].sum()\n\n#print(df.iloc[[4339]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84cc01ccc85f1b6dcde422c4c30187da759d51fe"},"cell_type":"code","source":"#changing the datetime format.\ndf['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%dT%H:%m:%s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2660142ecf04493fbc3d7dfcf970ad401e531e8f"},"cell_type":"code","source":"#Changing the data types\ndf[['bedrooms', 'bathrooms']] = df[['bedrooms', 'bathrooms']].astype(float)\ndf[['sqft_lot','sqft_living']] = df[['sqft_lot','sqft_living']].astype(np.int64)\ndf['price'] = df['price'].apply(np.int64)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a35b2d6ca206c6e1a73e08562044b9ac833ff52e"},"cell_type":"code","source":"#rearranging the dataframe according to the requirement\ndf = df[['date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', \n         'condition', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'street', 'city', 'statezip', \n         'country']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26d8cebabf4493645f957a1d9b2b10e616ebc437"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"affe786ca412eb21e3fa006ab68f94c99b5604da"},"cell_type":"markdown","source":"# Auditing and cleansing the loaded data\nIn this task, we are inspecting and auditind the data to identify the data problems, and then fix the problems. Different generic and major data problems could be found in the data might include:\n\n* Lexical errors, e.g., typos and spelling mistakes\n* Irregularities, e.g., abnormal data values and data formats\n* Violations of the Integrity constraint.\n* Outliers\n* Duplications\n* Missing values\n* Inconsistency, e.g., inhomogeneity in values and types in representing the same data"},{"metadata":{"trusted":true,"_uuid":"21ccc727c176d9826741c40c7fdb5f77d08d6b7c"},"cell_type":"code","source":" # 'O' for Objects\n#df.describe(include=['O'])\ndf.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1cdfaa1d9e72192369e6c12e941681b503f30b3"},"cell_type":"markdown","source":"###  Investigating Lexical errors for all the columes in dataframe"},{"metadata":{"trusted":true,"_uuid":"38f42e6f7d11433ef41c15b8085e82f8a7709ffa"},"cell_type":"code","source":"df['city'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"335c7f9f5712ce7bd33a3ee4f9dfdf46b0c264ee"},"cell_type":"markdown","source":"From the above data, when we observe the unique values, Some of the city names were Lexical errors."},{"metadata":{"trusted":true,"_uuid":"ca955123fce4cbae389cb09f45e6974b472c1d03"},"cell_type":"code","source":"#City\n# there is a Lexical error in city \"sammamish\" which is replaced with the average value of city named \"Sammamish\"\ndf['city'].replace(['sammamish'],['Sammamish'],inplace=True) \n\n# there is a Lexical error in city \"Samamish\" which is replaced with the average value of city named \"Sammamish\"\ndf['city'].replace(['Samamish'],['Sammamish'],inplace=True) \n\n# there is a Lexical error in city \"Seaattle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['Seaattle'],['Seattle'],inplace=True) \n\n# there is a Lexical error in city \"Seatle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['Seatle'],['Seattle'],inplace=True) \n \n# there is a Lexical error in city \"seattle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['seattle'],['Seattle'],inplace=True)\n\n# there is a Lexical error in city \"Issaguah\" which is replaced with the average value of city named \"Issaquah\"\ndf['city'].replace(['Issaguah'],['Issaquah'],inplace=True)\n\n# there is a Lexical error in city \"Woodenville\" which is replaced with the average value of city named \"Woodinville\"\ndf['city'].replace(['Woodenville'],['Woodinville'],inplace=True)\n \n# there is a Lexical error in city \"redmond\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['redmond'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"Redmund\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['Redmund'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"Redmund\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['Redmonde'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"auburn\" which is replaced with the average value of city named \"Auburn\"\ndf['city'].replace(['auburn'],['Auburn'],inplace=True)\n\n# there is a Lexical error in city \"Auburnt\" which is replaced with the average value of city named \"Auburn\"\ndf['city'].replace(['Auburnt'],['Auburn'],inplace=True)\n\n# there is a Lexical error in city \"Sureline\" which is replaced with the average value of city named \"Shoreline \"\ndf['city'].replace(['Sureline'],['Shoreline'],inplace=True)\n\n# there is a Lexical error in city \"Bellvue\" which is replaced with the average value of city named \"Bellevue \"\ndf['city'].replace(['Bellvue'],['Bellevue'],inplace=True)\n\n# there is a Lexical error in city \"Belleview\" which is replaced with the average value of city named \"Bellevue \"\ndf['city'].replace(['Belleview'],['Bellevue'],inplace=True)\n\n# there is a Lexical error in city \"Snogualmie\" which is replaced with the average value of city named \"Snoqualmie\"\ndf['city'].replace(['Snogualmie'],['Snoqualmie'],inplace=True)\n\n# there is a Lexical error in city \"Coronation\" which is replaced with the average value of city named \"Carnation\"\ndf['city'].replace(['Coronation'],['Carnation'],inplace=True)\n\n# there is a Lexical error in city \"Kirklund\" which is replaced with the average value of city named \"Kirkland\"\ndf['city'].replace(['Kirklund'],['Kirkland'],inplace=True)\n\n#The above changes can aslo be done as show in below code,\n#df.city.replace({\"sammamish\":\"Sammamish\", \"Samamish\": \"Sammamish\", \"Seaattle\":\"Seattle\", \"Seatle\":\"Seattle\",\n#\"seattle\":\"Seattle\", \"Issaguah\":\"Issaquah\"}, inplace=True) \ndf.city.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ca8af2d04419b31afc8cf476c64b17626dafe30"},"cell_type":"code","source":"df['bathrooms'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ab9ee177cc3660ea10bb08769563758c12cfc9c"},"cell_type":"markdown","source":"From the above data, when we observe the unique values, Some of the \"bathrooms\" values were Irregular."},{"metadata":{"trusted":true,"_uuid":"b9208c0eeb8418ad008fef1e687dd16cd3dfbda3"},"cell_type":"code","source":"#Bathroom\n# there is a abnormal data value in \"bathrooms\", \"1.70\" which is replaced with the average value\"1.75\"\ndf['bathrooms'].replace([1.70],[1.75],inplace=True) \n\n# there is a lexical error in \"bathrooms\", \"1.05\" which is replaced with the value \"1.50\"\ndf['bathrooms'].replace([1.05],[1.50],inplace=True) \n\n# there is a abnormal data value in \"bathrooms\", \"2.55\" which is replaced with the nearest value of bathrooms named \"2.50\"\ndf['bathrooms'].replace([2.55],[2.50],inplace=True) \n\n# there is a abnormal data value in \"bathrooms\", \"2.30\" which is replaced with the average value of bathrooms named \"2.25\"\ndf['bathrooms'].replace([2.30],[2.25],inplace=True) \n\n# there is a lexical error in \"bathrooms\", \"2.57\" which is replaced with the average value of bathrooms named \"2.75\"\ndf['bathrooms'].replace([2.57],[2.75],inplace=True) \n\ndf['bathrooms'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bbc4d54188ca855cb054005973f9159b3729a2a"},"cell_type":"markdown","source":"### Investigating Duplicates"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e8f353bcb02fd438b04fc47b7e408328a4fc269e"},"cell_type":"code","source":"df[df.duplicated(keep=False)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9099c2d560f8c55c72153a36f47d261bffa59d3"},"cell_type":"markdown","source":"From the above data, we found that there is only one row which is repeating twice."},{"metadata":{"trusted":true,"_uuid":"7837ca761aa00a2f27a2965063fac385d1eac600"},"cell_type":"code","source":"#dropping the row which are duplictaes\ndf.drop_duplicates(keep=\"first\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc37cc42a24fa2d10f234058fad4fdf9216e978d"},"cell_type":"markdown","source":"### Investigating the missing values "},{"metadata":{"_uuid":"219eb59d058045fcabbb4a11e03946c2755fb938"},"cell_type":"markdown","source":"'nan' doesn't occur in counts() \n\n*  checking how many 'NaN' values are there\n*  checking how many 0\n*  checking how many < 1"},{"metadata":{"trusted":true,"_uuid":"54da7d7f5ca6bbe95c56bef3e18645df9094d176"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c7bdaeb9b3d5efc8baec0a889b1968ed8b029be"},"cell_type":"markdown","source":"There are 4601 x 18 records and there are many missing values in \"yr_renovated\""},{"metadata":{"trusted":true,"_uuid":"e85388aebeb2fd30af96119fb08804adf92c6616"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5be9c378e888801f1f2ca57e5d74f5587c98a5c"},"cell_type":"markdown","source":"From the above data, we see that there are 4371 null values in \"yr_renovated\""},{"metadata":{"trusted":true,"_uuid":"8ce95efaea52d7a016876a52d1af7980622c3dab"},"cell_type":"code","source":"# unique values of 'yr_renovated'\ndf['yr_renovated'].unique()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7d08ac8d1ecfbe1808d823720b0ce4feed9a7116"},"cell_type":"code","source":"#creating dummy dataframe to use it for predicting the null values for yr_renovated by using mean\ndf_impute= df.copy()\ndf_impute.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"201e419056e0984f7fe0447f4dd21d66122b5162"},"cell_type":"markdown","source":"##### There are two ways that we are trying to replace the null values of \"yr_renovated\"\n* Replacing all NaN with zero \n* Replacing NaN with mean"},{"metadata":{"trusted":true,"_uuid":"51b264b72466a0176a8e6e0470fc6a62f6242054"},"cell_type":"code","source":"#converting the yr_renovated column, from float64 to int64 and replacing all NaN to '0'\ndf_impute['yr_renovated'] = np.nan_to_num(df_impute['yr_renovated']).astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e56610f98f26ed3eb8f85b0edb5744007f40942"},"cell_type":"markdown","source":"##### Predicting the yr_renovated when changed from NaN to zero, with the full data"},{"metadata":{"trusted":true,"_uuid":"0969becd25cdd0c68562ff89ed65afc23ed2ff04"},"cell_type":"code","source":"#For linear regression we use sklearn (built in python library) and import linear regression from it.\nfrom sklearn.linear_model import LinearRegression\n#Initializing Linear Regression to a variable reg\nreg = LinearRegression()\n#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_impute['yr_renovated']\n#Converting dates to 1’s and 0’s so that it doesn’t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_impute.date]\ndf_impute['date'] = conv_dates\ntrain1 = df_impute.drop(['city','street','country','statezip','price'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3546ce67d740409726bb80389b6220887b1bf20f"},"cell_type":"code","source":"#We again import another dependency to split our data into train and test\nfrom sklearn.cross_validation import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"705562440aed436624c2ea3db0385d7e021ec3d0"},"cell_type":"code","source":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17abfcb202fc07aaa3b439a5ff7e730138aa056f"},"cell_type":"code","source":"reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e3ed77a230e167934239c8114e24e360e98b9a4"},"cell_type":"code","source":"reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ad05862b5f111582e904dc6d6ce2fd0d22c94f"},"cell_type":"markdown","source":"##### Predicting the yr_renovated when NaN is changed with mean values with the full data"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a0fa353073cdb262062ed1436ec0678979d384bd"},"cell_type":"code","source":"df[\"yr_renovated\"].fillna(df.groupby([\"yr_built\",\"condition\"])[\"yr_renovated\"].transform(\"mean\"), inplace=True)\ndf.yr_renovated.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"536218e069ed240c77353a4bd7153ae520d62032"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87008ca249bcf8d6986a8917b09bdbf34aa0c613"},"cell_type":"code","source":"df['yr_renovated'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bed6e5c07ccd45044b16ffa81c358ba930f4af3"},"cell_type":"code","source":"df_impute = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdd76cbf405b2565b0252eb8d34b1c7fd783bf51"},"cell_type":"code","source":"#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_impute['yr_renovated']\n#Converting dates to 1’s and 0’s so that it doesn’t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_impute.date]\ndf_impute['date'] = conv_dates\ntrain1 = df_impute.drop(['city','street','country','statezip','price'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65aea3802a695cdc87a3bf5e00345c51194b1dc8"},"cell_type":"code","source":"reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eab26bf5c63d25619bf9245de7790aab7c09128a"},"cell_type":"code","source":"reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9959171165b61666fd8cb91190f243e6b8d8b3e7"},"cell_type":"markdown","source":"##### Predicting the yr_renovated without any changes with the full data"},{"metadata":{"trusted":true,"_uuid":"524bd29a4cb5a1d774b3f39390945d9c0350daff"},"cell_type":"code","source":"df_imptd1 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3609d8ced59bbba89f3a28f89f026435364e85eb"},"cell_type":"code","source":"#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_imptd1['yr_renovated']\n#Converting dates to 1’s and 0’s so that it doesn’t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_imptd1.date]\ndf_imptd1['date'] = conv_dates\ntrain1 = df_imptd1.drop(['city','street','country','statezip','price'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"240247ba46d1008bdd4b040942d960dda907d569"},"cell_type":"code","source":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84171700981f3998d057c96863559eb646315c0e"},"cell_type":"code","source":"reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74ca44b6d63ba4daaba2cbf81f7deb8c3e884be6"},"cell_type":"code","source":"reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"369c4643cad07619dfba0ada717645a8b3be11f0"},"cell_type":"code","source":"df['yr_renovated'] = df_impute['yr_renovated'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd73b8e477ce497c5b934f0af5e3b500ccdbe6c2"},"cell_type":"markdown","source":"###### checking how many 0 in price"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ae3aeb8341c4d440e127b86184305ad6dbfb71a0"},"cell_type":"code","source":"df.price.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e83d07f5a5b9f0b23628b79962f159fb2f50a65"},"cell_type":"markdown","source":"From the above observation, we see that there are 248 count of \"0.0\" in price\n"},{"metadata":{"_uuid":"5fb06aa79c0b9e89ffb844d43d25328d0e25e172"},"cell_type":"markdown","source":"##### checking how many values are < 1"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"82a0fcd44647b521737a9877f6e0c03df76b958a"},"cell_type":"code","source":"#checking how many values are < 1\ndf_impute[df_impute['price'] < 1] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf3e2000b6203934e0b6971bf3a3cf3a3376b8d5"},"cell_type":"code","source":"df[[\"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"sqft_above\",\"yr_built\",\"sqft_living\",\"sqft_lot\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1928f5d52997e496f40d9c933588d4664604aae"},"cell_type":"markdown","source":"We can see the home prices vary from $0 to $2.659000e+07 with living space from 370.000000sqft to 13540sqft. Lots of variety!"},{"metadata":{"_uuid":"3e397ebeb77a6f540821ab29b8e8a2125810063c"},"cell_type":"markdown","source":"###### We are trying to replace the zero values of \"price\"\nReplacing zero with mean"},{"metadata":{"trusted":true,"_uuid":"e0ee7a1644e1b308c847fa5965e687f30d05560d"},"cell_type":"code","source":"#replacing all the 0.0 to NaN\ndf['price'] = df['price'].replace(0.0, np.nan)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"59733cfbd4cce10154e2ab36333ae17ed922fa4b"},"cell_type":"code","source":"#replacing all NaN to mean values of price \ndf[\"price\"].fillna(df_impute.groupby([\"bedrooms\",\"bathrooms\",\"city\",\"statezip\"])[\"price\"].transform(\"mean\"), inplace=True)\ndf.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"022923fb8e09748eb7afd0e5d607febe5c010c47"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11b232825d098e3ef941ba3944abe4cded56f0ec"},"cell_type":"code","source":"#dropping all NaN values from price\ndf.dropna(subset=['price'],axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a9b1c0da74a7fad28cb0e5ef801037c4a72f8d2"},"cell_type":"code","source":"df_final = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c7396c30e297a516fe55d6b9e47e68cd0390a84"},"cell_type":"markdown","source":"### Using boxpot to detect outliers"},{"metadata":{"trusted":true,"_uuid":"892dec6d08d80b2d6fc70259eb729b6093214230"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndf.boxplot(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e33231f21c362c9fe46662b4957cce0d56884a4a"},"cell_type":"markdown","source":"The first thing to notice is that 'price' has many outliers.\nHowever, plotting all data together might not be right because of the different ranges of attributes. Therefore, we look at one attribute at a time instead."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"5c8b116960deb6a3656178a50e7d5891e5224bca"},"cell_type":"code","source":"bp = df.boxplot(column='price',figsize=(10,15))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ccbfe84439d22b1634bd4aad42201e729ea5c0d4"},"cell_type":"code","source":"# We can see a bunch of price above 0.5, then something around 2.5, the outliers are:\ndf[df['price'] > 2.0] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3d4ec4ac417fe9fef09a46ea3f6fa0176d4ffb7"},"cell_type":"markdown","source":"price of 26590000.0 and 12899000.0 looks very high compared to other, that's a weird value too and price of 7800.0 and 84350.0 \nlooks very low compared to other values.\n* Price also depends on the other conditions of the house\n* Lets check the price according to \"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"sqft_above\",\"yr_built\",\"sqft_living\",\"sqft_lot\"\n\n###### Let's investigate outliers by each attribute "},{"metadata":{"trusted":true,"_uuid":"35cec60bccb63452a79be9920b8aabe11fe995ed"},"cell_type":"code","source":"# plotting baoxplot to check outliers price vs bedrooms\nbp = df.boxplot(column='price', by = 'bedrooms',figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"310c3bba58282ba02dc3129434ec9519f9fbdb7e"},"cell_type":"markdown","source":"From the above graph, we can summarize that for a standard three bedroom house, the number of outliers imply that the owners paid a premium price for their respective property, which would have either been at a waterfront or the properties' per sqft_living value would have been higher than standard rates. Figuratively, the sandard rates are below 0.5. Under close observation, above the value of 1.0, there are few properties which may have higher sqft_living value, the values above 2.5 might be of those properties which may be near water front \n\nTo verify the above properties whose sqft_living or their location is at a waterfront, we further plot refined graph on waterfront, sqft_living, bedrooms, bathrooms, view, condition. "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ea9d9e0e9648b30b69faf5be7a95b2e11d86f688"},"cell_type":"code","source":"df[df['price'] > 2.0] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8c8c6198f2e8ec63aa1417c351df5f21be6e85dc"},"cell_type":"code","source":"#creating a dummy dataframe to check on price vs date of property sold.\ndf_dummy = df\n#df_dummy['date'] = pd.to_datetime(df_dummy['date'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b0961f70647bf7042ff88075aa69737a2c51bc7e"},"cell_type":"code","source":"# plot price vs date\n\n#df_dummy['Year'] = df_dummy['date'].dt.year\n#df_dummy.boxplot(column='price', by='Year', figsize=(10,10))\n\n#df.set_index(['date',df.date.dt.year])['price'].unstack().boxplot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f053c3d20cd286171a70629d300ca2e0a56dd6d7"},"cell_type":"markdown","source":"We can see a bunch of price above 0.5, then something around 1.0 and then something around 2.5"},{"metadata":{"trusted":true,"_uuid":"288f58669a159bc1702170ebb9d17bb42218c6e4"},"cell_type":"code","source":"df[(df['price'] > 0.5) & (df['price'] > 1.0) & (df['price'] > 2.5)].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77eb14d05c0f25596b9ce588f5d9888c48f77d93"},"cell_type":"markdown","source":"price of minimum 7.800000e+03 with sqft_living 370.000000, zero bathroom and zero bedroom with condition as 1 which is really low comparing to other property's conditions, 2.659000e+07 with sqft_living 13540.000000 looks very high compared to other, that's a weird value too and maximum price of 2.659000e+07 and 7.800000e+03 looks very low compared to other values.\nPrice also depends on the other conditions of the house"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2d51d0306075fbc01178020763eec59c7b4caca5"},"cell_type":"code","source":"# sqft_lot\ndf.boxplot(column='sqft_lot', figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccf381d384c31ee35c05e6dbd548f8c15461ab11"},"cell_type":"markdown","source":"We can see a bunch of sqft_lot above 400000, then something around 600000, look at the outliers:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"420267eb323826cbca006f575d3296f34cba3632"},"cell_type":"code","source":"df[(df['sqft_lot'] > 400000) & (df['sqft_lot'] > 600000)] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82ddb40ca1fbfa49ba4de4d43753fe615ca62398"},"cell_type":"markdown","source":"We can see a bunch of sqft_lot above 1074218, then something around 600000, look at the outliers:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4bb3a05b222c333e1d7ce0a7802e0d256b7e4ab1"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nfrom matplotlib import rcParams\n\n%matplotlib inline \n%pylab inline ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8186e361b372c2712fec9577970f7f9985dc2078"},"cell_type":"markdown","source":"Lets look at \"continuous\" features. Intuitively that will be sqft_living but could possibly be 'bathrooms','bedrooms','sqft_living','sqft_lot','sqft_above','waterfront'. Lets take a look at these with some plots using seaborn."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8d2f8b05e6f91ce6086758da4ead2652ef18d277"},"cell_type":"code","source":"sns.pairplot(data=df, x_vars=['bathrooms','bedrooms','sqft_living','sqft_lot','sqft_above','waterfront'], y_vars=[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"952e369d1e9ccc21af940b6ea61fb590841867b1"},"cell_type":"markdown","source":"We can see that \"lot\" size is not well correlated to price but the data for living space is reasonable. Visually the best feature to use looks like sqft_living as we expected."},{"metadata":{"_uuid":"5abae9460302627e05d0fd6bbca1b9ca41ed1038"},"cell_type":"markdown","source":"## Multivariate linear regression"},{"metadata":{"_uuid":"25aa5c763512a31fe8b7da6787a4454a7fda2bf7"},"cell_type":"markdown","source":"It’s important to look at the shape of the data – and to double check if the data is reasonable. Corrupted data is not uncommon,so will run two checks\n* first, use df.describe() to look at all the variables in our analysis. \n* Second, plot histograms of the variables that the analysis is targeting using plt.pyplot.hist()."},{"metadata":{"trusted":true,"_uuid":"7063729182e1321363c212138a411ebf70c790f4"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f300c9b41741828c33945b44759f8319b3576c20"},"cell_type":"markdown","source":"Quick takeaways: We are working with a dataset that contains 4600 observations, mean price is approximately $5.648432e+05,median price is approximately $4.710000e+05, and the average house’s area is 2139.346957 ft2"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cd783871e06139881d057304de38dc9b4cefbc8a"},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06a257309bfd78497e6fda9ad47f13a3bf4ca16"},"cell_type":"markdown","source":"When we produce a linear regression summary with OLS with only two variables this will be the formula that we use:\n\nReg = ols(‘Dependent variable ~ independent variable(s), dataframe).fit()\n\nprint(Reg.summary())\n\nWhen we look at housing prices and square footage for houses, we print out the following summary report:"},{"metadata":{"trusted":true,"_uuid":"1c2ac7fba2aab2ed320dad96bc61f4b2db7dac0a"},"cell_type":"code","source":"m = ols('price ~ sqft_living',df).fit()\nprint (m.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1d68d591bbdaf71c710a81db914d2254097207f"},"cell_type":"markdown","source":"When we print the summary of the OLS regression, all relevant information can be easily found, including R-squared, t-statistics, standard error, and the coefficients of correlation. Looking at the output, it’s clear that there is an extremely significant relationship between square footage and housing prices since there is an extremely high t-value of 144.920, and a P>|t| of 0%–which essentially means that this relationship has a near-zero chance of being due to statistical variation or chance.\n\nThis relationship also has a decent magnitude – for every additional 100 square-feet a house "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"da3d4ae85e36b1faf87d23cd21a3baa8ecc4e40f"},"cell_type":"code","source":"m = ols('price ~ sqft_living + bedrooms + view + condition',df).fit()\nprint (m.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"509204923e519581356cbc7b9c4ebe13b123621e"},"cell_type":"markdown","source":"In our multivariate regression output above, we learn that by using additional independent variables, such as the number of bedrooms, we can provide a model that fits the data better, as the R-squared for this regression has increased to 0.222. This means that we went from being able to explain about 0.202(20.2%) of the variation in the model to 0.222(22.2%) with the addition of a few more independent variables. "},{"metadata":{"_uuid":"b8a0e98695206202085c204129f96771851eb038"},"cell_type":"markdown","source":"Having the regression summary output is important for checking the accuracy of the regression model and data to be used for estimation and prediction – but visualizing the regression is an important step to take to communicate the results of the regression in a more digestible format.\n\nThis section will rely entirely on Seaborn (sns) , which has an incredibly simple and intuitive function for graphing regression lines with scatterplots. I chose to create a jointplot for square footage and price that shows the regression line as well as distribution plots for each variable."},{"metadata":{"trusted":true,"_uuid":"c9484748d7ce9f91a47cd22d20229502a099b794"},"cell_type":"code","source":"sns.jointplot('sqft_living','price', data=df, size=10, alpha=.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bbf7732bd015c8fab5947d7f1ed564c1199dd64"},"cell_type":"markdown","source":"The increase of price with sqft_living space is pretty clear and the \"Pearson r value\" is 0.45 indicating a reasonable correlation. However, the data distributions show a big concentration of values in the lower left of the plot. That makes sense, most houses are between 300 and 3000 sqft and a few hundred thousand dollars. We can eliminate the very expensive and very large houses and take another look at the data.\n\nIf we set the size (xlim) from 500 to 3500sqft and the price (ylim) from 100,000 to $1,000,000 the data still shows the trend but it looks very scattered."},{"metadata":{"trusted":true,"_uuid":"83f189c344046b19f87ed4371637c83e860f3755"},"cell_type":"code","source":"sns.jointplot('sqft_living','price', data=df, xlim=(500,3500), ylim=(100000,1000000), size=10, alpha=.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77a3549b8141ee5afd12c15513aa626552683d00"},"cell_type":"markdown","source":"Something worth considering is that different neighborhoods can vary greatly in average house price. Some nice neighborhoods are very expensive and some other (also nice!) neighborhoods can be quite affordable. It might be good to look at average house price by zipcode since we have that in our dataset."},{"metadata":{"trusted":true,"_uuid":"4693b4265aa3c6a7ae5951be8c24bcef4ff6626a"},"cell_type":"code","source":"df[\"statezip\"].nunique()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44e4c8398ce1cd8de552e26bd7ea1c76a6e0fdcb"},"cell_type":"markdown","source":"It looks like there are 77 different statezip code in the given data. Lets see how many house sales there were in each."},{"metadata":{"trusted":true,"_uuid":"3ff2ebafbc27a99db32b2deaa40816027c8c60d4"},"cell_type":"code","source":"df['statezip'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfe199d2add6b2b791441094a488a34d1d4f0783"},"cell_type":"markdown","source":"Trying to find the average house sale price in each zipcode ..."},{"metadata":{"trusted":true,"_uuid":"e55f7b99e96bf413e7f8435f5a49371958b9a5ae"},"cell_type":"code","source":"df.groupby('statezip')['price'].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff2c3e2aa7dce86dab6243333fd4c89cef72bad2"},"cell_type":"markdown","source":"The zipcode that look the most interesting to me is WA 98103. WA 98103 has the most house sale values, 148, with an average sale price of $5.603248e+05. \n"},{"metadata":{"trusted":true,"_uuid":"87d1f1e4799e3a832769e9e53d303ac3c39ce85a"},"cell_type":"code","source":"zip_WA_98103 = df['statezip'] == \"WA 98103\"  # True if zip is 98103","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d9bfa27b8823c8ea93cac4f23c3661a6868b605"},"cell_type":"markdown","source":"Using the \"selectors\" above we can look at plots of price vs sqft_living in that zipcode."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4c11d3ffebf5b8ab6d70b5d8f3cb733c9d1c3f37"},"cell_type":"code","source":"sns.jointplot('sqft_living','price', data=df[zip_WA_98103], size=10, alpha=.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0897fab657589d1ea14366cf186fad54310c5905"},"cell_type":"markdown","source":"The 98103 zipcode has a distribution that looks similar to the complete dataset."},{"metadata":{"trusted":true,"_uuid":"d83df390c9631fc00a2faec417ce9598b3991eb4"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f405703eb6a8cab1e6a50661ff106d7cabec9bac"},"cell_type":"markdown","source":"# Working with Regression Model\n\nLooking at the bedroom columns , the dataset has a house where the house has 9 bedrooms , seems to be a massive house and would be interesting to know more about it as we progress.\n\nMaximum square feet is 13,540 where as the minimum is 370. we can see that the data is distributed.\nSimilarly , we can infer so many things by just looking at the describe function.\n\nNow , we are going to see some visualization and also going to see how and what can we infer from visualization.\n\nLet’s see which is most common bedroom number. Let’s look at this problem from a builder’s perspective, sometimes it’s important for a builder to see which is the highest selling house type which enables the builder to make house based on that. For example, in India , for a good locality a builder opts to make houses which are more than 3 bedrooms which attracts the higher middle class and upper class section of the society.\nLet’s see how this pans out for this data!\n"},{"metadata":{"trusted":true,"_uuid":"3cbc3ae073266d821125489814e8784f6f9fbd77"},"cell_type":"code","source":"import seaborn as sns\nimport mpl_toolkits\n\n#\ndf['bedrooms'].value_counts().plot(kind='bar')\nplt.title('Number of Bedrooms')\nplt.xlabel('Bedrooms')\nplt.ylabel('Count')\nsns.despine\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6e189b648b1c938340db3efa28759473de3f0b"},"cell_type":"markdown","source":"As we can see from the visualization 3 bedroom houses are most commonly sold followed by 4 bedroom. So how is it useful ? For a builder having this data , He can make a new building with more 3 and 4 bedroom’s to attract more buyers.\nSo now we know that 3 and 4 bedroom’s are highest selling. But at which locality ?\n#### How common factors are affecting the price of the houses ?\nWe saw the common locations and now we’re going to see few common factors affecting the prices of the house and if so ? then by how much ?\nLet us start with , If price is getting affecting by living area of the house or not ?"},{"metadata":{"trusted":true,"_uuid":"96a58c5d8de2c501767a7661cda689a75523bf04"},"cell_type":"code","source":"#PRIcE Vs SQFT_LIVING\nplt.scatter(df.price,df.sqft_living)\nplt.title(\"Price Vs Square feet\")\nplt.xlabel('Square feet')\nplt.ylabel('Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9d6fd72307c61a52bd8c381e8d69c7d1ed63f2c"},"cell_type":"markdown","source":"From the above figure we can see that more the living area , more the price though data is concentrated towards a particular price zone , but from the figure we can see that the data points seem to be in linear direction.We can also see some irregularities that the house with the highest square feet was sold for very less , maybe there is another factor or probably the data must be wrong. "},{"metadata":{"trusted":true,"_uuid":"74ffb1cb6fa8d31ca86a849f6c9bc7997fcdf6ab"},"cell_type":"code","source":"#PRICE Vs BEDROOMS\nplt.scatter(df.bedrooms,df.price)\nplt.title(\"Bedroom and Price\")\nplt.xlabel(\"Bedrooms\")\nplt.ylabel(\"Price\")\nplt.show()\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f8bbc78fc1a85809ae0fe6d81358653cdf33aea"},"cell_type":"markdown","source":"We can see more factors affecting the price"},{"metadata":{"trusted":true,"_uuid":"dd2b70d4368d5a79f68ff80013aefeb68316b1fb"},"cell_type":"code","source":"#Total sqft including basement vs price and waterfront vs price\nplt.scatter((df['sqft_living']+df['sqft_basement']),df['price'])\nplt.title(\"sqft_living and sqft_basement vs Price\")\nplt.xlabel(\"sqft_living and sqft_basement\")\nplt.ylabel(\"Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0368bbc1fc2aa0c6d884a689db2111d5e52cb012"},"cell_type":"code","source":"plt.scatter(df.waterfront,df.price)\nplt.title(\"Waterfront Vs Price (0 = No Waterfront )\")\nplt.xlabel(\"waterfront\")\nplt.ylabel(\"Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9815d116287b73ee399e67d1cd7d9057ef26829e"},"cell_type":"markdown","source":"##### Floors vs Price and condition vs Price"},{"metadata":{"trusted":true,"_uuid":"c86b644b5afba9aa2bfdbfa516c7255867c6c566"},"cell_type":"code","source":"plt.scatter(df.floors,df.price)\nplt.title(\"floors Vs Price\")\nplt.xlabel(\"floors\")\nplt.ylabel(\"Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3168e604336f87d4ca87538141f6710627a75849"},"cell_type":"code","source":"plt.scatter(df.condition,df.price)\nplt.title(\"condition Vs Price\")\nplt.xlabel(\"condition\")\nplt.ylabel(\"Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c87cef7ba5caa580c9bafb258432aa5ed3bae8c"},"cell_type":"markdown","source":"As we can see from all the above representation that many factors are affecting the prices of the house , like square feet which increases the price of the house and even location influencing the prices of the house.\nCreating a model to which would predict the price of the house based upon the other factors such as square feet , water front etc ."},{"metadata":{"trusted":true,"_uuid":"e18c1e1182d95539f99359fb9bddeb4a5a9f2426"},"cell_type":"code","source":"#For linear regression we use sklearn (built in python library) and import linear regression from it.\nfrom sklearn.linear_model import LinearRegression\n#Initializing Linear Regression to a variable reg\nreg = LinearRegression()\n#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df['price']\n#Converting dates to 1’s and 0’s so that it doesn’t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df.date]\ndf['date'] = conv_dates\ntrain1 = df.drop(['city','street','country','statezip','price'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f476088aa7ffcfbc1b338d0c863e462257ac626c"},"cell_type":"code","source":"#We again import another dependency to split our data into train and test\nfrom sklearn.cross_validation import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"176b9282fb36d0282393eec3850383175d85ff55"},"cell_type":"code","source":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54d220d5e56dbcac6bc041f9e3a210e4569c04fa"},"cell_type":"code","source":"map(pd.np.shape,[X_train, X_test, y_train, y_test])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9a9c8548b46f5c4a2ec4127471f6a0527b38ad5"},"cell_type":"markdown","source":"We have train data , test data and labels for both. Fitting our train and test data into linear regression model."},{"metadata":{"trusted":true,"_uuid":"ad0ae7297eccb428edb4661a89bce4440b0576da"},"cell_type":"code","source":"reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fd36c8018cc957dc5a3333310307d51933b8599"},"cell_type":"code","source":"reg.score(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"409598567071bedb4796c596dc49c5b55e943ef4"},"cell_type":"markdown","source":"After fitting our data to the model we can check the score of our data ie , prediction. in this case the prediction is 58%"},{"metadata":{"trusted":true,"_uuid":"cca133f4cadc557dae56c7fa307593ce0c1cded2"},"cell_type":"code","source":"df_final.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86b539a3e32c0316dba4827f1f59012744b55cbd"},"cell_type":"code","source":"df_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71076be2912313268d683a2ebeadb16c38dc64b5"},"cell_type":"code","source":"filename = 'output.csv'\ndf_final.to_csv(filename, encoding='utf-8', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46db87af97d3b297e28ff5eb7ae38e32d54823ac"},"cell_type":"markdown","source":"##### References:\n* https://stackoverflow.com/\n* https://www.coursera.org/learn/ml-regression/lecture/G12Qp/a-case-study-in-predicting-house-prices\n* https://medium.com/towards-data-science/create-a-model-to-predict-house-prices-using-python-d34fe8fad88f\n* https://www.datacamp.com/courses/cleaning-data-in-python\n* http://www.developintelligence.com/blog/2017/08/data-cleaning-pandas-python/\n* http://uwescience.github.io/DSSG2015-predicting-permanent-housing/images/DSSG2015-PPH-final-presentation.pdf\n"},{"metadata":{"trusted":true,"_uuid":"aea7aea78f030503a8ddb3ed3ad17e2d86a372dc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"}},"nbformat":4,"nbformat_minor":1}